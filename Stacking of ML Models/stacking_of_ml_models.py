# -*- coding: utf-8 -*-
"""Stacking of ML Models.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16T4rVFaTTGIu1Ock1mtYlGcxzAkJFXzT
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier

from sklearn.metrics import plot_confusion_matrix, classification_report, matthews_corrcoef, accuracy_score, f1_score

data = pd.read_csv("/content/ObesityDataSet_raw_and_data_sinthetic.csv")

data

data.info()

data.isna().sum()

print(data['family_history_with_overweight'].unique())
print(data['FAVC'].unique())
print(data['CAEC'].unique())
print(data['SMOKE'].unique())	
print(data['SCC'].unique())
print(data['CALC'].unique())
print(data['MTRANS'].unique())
print(data['NObeyesdad'].unique())

"""## So we have:
- 8 columns that are categorical including target column
- 3 are more than 2 types of records so we need to one-hot encode them (Only considering features)
- Rest 4 we can replace the categories into 0 and 1.
"""

data['NObeyesdad'].value_counts()

"""## Good distribution of data in the target column"""

data['NObeyesdad'].value_counts().plot(kind='barh')

"""## Multiclass Classification Problem

## Preparing the data
"""

# One hot encode function
def one_hot_encoding(df, column, rename=False):
  df = df.copy()
  if rename == True:
    df[column] = df[column].replace({x: i for i, x in enumerate(df[column].unique())})
  dummies = pd.get_dummies(df[column], prefix=column)
  df = pd.concat([df, dummies], axis=1)
  df = df.drop(column, axis=1)

  return df

def preprocess_inputs(df):
  df = df.copy()

  #Replacing categorical column records with 0 and 1 (Only binary columns)
  df['Gender'] = df['Gender'].replace(['Male', 'Female'], [1,0])
  df['family_history_with_overweight'] = df['family_history_with_overweight'].replace(['yes', 'no'], [1,0])
  df['FAVC'] = df['FAVC'].replace(['yes', 'no'], [1,0])
  df['SMOKE'] = df['SMOKE'].replace(['yes', 'no'], [1,0])
  df['SCC'] = df['SCC'].replace(['yes', 'no'], [1,0])

  #One hot encode the 3 columns
  df = one_hot_encoding(df, column='CAEC', rename=False)
  df = one_hot_encoding(df, column='CALC', rename=False)
  df = one_hot_encoding(df, column='MTRANS', rename=False)

  # X and y
  X = df.drop("NObeyesdad", axis=1)
  y = df['NObeyesdad']

  #split
  X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.8, shuffle=True, random_state=21)
 
  return X_train, X_test, y_train, y_test

"""### Tree based models don't require scaled data so not scaling the data as we are going to use tree-based classifiers."""

X_train, X_test, y_train, y_test = preprocess_inputs(data)

X_train

X_train.info()

y_train

print(len(X_train))
print(len(X_test))
print(len(y_train))
print(len(y_test))

"""## Training the Model"""

dtree = DecisionTreeClassifier()
dtree_clf = dtree.fit(X_train, y_train)

random_forest = RandomForestClassifier()
random_forest_clf = random_forest.fit(X_train, y_train)

xgboost = GradientBoostingClassifier()
xgboost_clf = xgboost.fit(X_train, y_train)

"""# Evaluation and Interpretibility

# Decision Tree
"""

#get predictions
y_train_pred = dtree_clf.predict(X_train)
y_test_pred = dtree_clf.predict(X_test)

#Training set performances
dtree_clf_train_accuracy = accuracy_score(y_train, y_train_pred) # Calculating Accuracy
dtree_clf_train_mcc = matthews_corrcoef(y_train, y_train_pred) # Calculating MCC
dtree_clf_train_f1 = f1_score(y_train, y_train_pred, average='weighted') # Calculating F1-score

#Test set performances
dtree_clf_test_accuracy = accuracy_score(y_test, y_test_pred) # Calculating Accuracy
dtree_clf_test_mcc = matthews_corrcoef(y_test, y_test_pred) # Calculating MCC
dtree_clf_test_f1 = f1_score(y_test, y_test_pred, average='weighted') # Calculating F1-score

print('Model performance for Training set')
print('- Accuracy: %s' % dtree_clf_train_accuracy)
print('- MCC: %s' % dtree_clf_train_mcc)
print('- F1 score: %s' % dtree_clf_train_f1)
print('----------------------------------')
print('Model performance for Test set')
print('- Accuracy: %s' % dtree_clf_test_accuracy)
print('- MCC: %s' % dtree_clf_test_mcc)
print('- F1 score: %s' % dtree_clf_test_f1)

plot_confusion_matrix(dtree_clf, X_test, y_test, labels = dtree_clf.classes_)

dtree_clr = classification_report(y_test, y_test_pred, labels=dtree_clf.classes_)
print(dtree_clr)

"""#### Feature importance"""

!pip install shap

import shap

# Calculate Shap values
explainer_dtree = shap.TreeExplainer(dtree_clf)
shap_values_dtree = explainer_dtree.shap_values(X_test)
shap.initjs()
shap.summary_plot(shap_values_dtree, X_test, class_names=dtree_clf.classes_)

"""## We can see "Weight", "Height", "Gender", and Age are few of the columns that have more impact on the model performance.

## Drawing the Decision Tree plot.
"""

from IPython.display import Image
from sklearn import tree
import pydotplus

X_train.columns

dot_data = tree.export_graphviz(dtree_clf,
                                out_file=None,
                                feature_names=X_train.columns,
                                class_names=dtree_clf.classes_)

graph = pydotplus.graph_from_dot_data(dot_data)

Image(graph.create_png())

#save the image
graph.write_png("decision_tree_plot.png")

"""# RANDOM FOREST (EVALUATION)"""

#get predictions
y_train_rf_pred = random_forest_clf.predict(X_train)
y_test_rf_pred = random_forest_clf.predict(X_test)

#Test set performances
random_forest_clf_test_accuracy = accuracy_score(y_test, y_test_rf_pred) # Calculating Accuracy
random_forest_clf_test_mcc = matthews_corrcoef(y_test, y_test_rf_pred) # Calculating MCC
random_forest_clf_test_f1 = f1_score(y_test, y_test_rf_pred, average='weighted') # Calculating F1-score

print('Model performance for Test set')
print('- Accuracy: %s' % random_forest_clf_test_accuracy)
print('- MCC: %s' % random_forest_clf_test_mcc)
print('- F1 score: %s' % random_forest_clf_test_f1)

plot_confusion_matrix(random_forest_clf, X_test, y_test, labels=random_forest_clf.classes_)

rf_clr = classification_report(y_test, y_test_rf_pred, labels=random_forest_clf.classes_)
print(rf_clr)

# Calculate Shap values
explainer_rf = shap.TreeExplainer(random_forest_clf)
shap_values_rf = explainer_rf.shap_values(X_test)
shap.initjs()
shap.summary_plot(shap_values_rf, X_test, class_names=random_forest_clf.classes_)

"""### FCVC is a important feature once random forest is in use. It was not the case in Decision Tree.

## XGBoost Evaluation
"""

#get predictions
y_train_xg_pred = xgboost_clf.predict(X_train)
y_test_xg_pred = xgboost_clf.predict(X_test)

#Test set performances
xg_test_accuracy = accuracy_score(y_test, y_test_xg_pred) # Calculating Accuracy
xg_test_mcc = matthews_corrcoef(y_test, y_test_xg_pred) # Calculating MCC
xg_test_f1 = f1_score(y_test, y_test_xg_pred, average='weighted') # Calculating F1-score

print('Model performance for Test set')
print('- Accuracy: %s' % xg_test_accuracy)
print('- MCC: %s' % xg_test_mcc)
print('- F1 score: %s' % xg_test_f1)

plot_confusion_matrix(xgboost_clf, X_test, y_test, labels=xgboost_clf.classes_)

xg_clr = classification_report(y_test, y_test_xg_pred, labels=xgboost_clf.classes_)
print(xg_clr)

"""# SHAP doesn't work for Gradient Boost when it is a multiclass problem. It only works for binary classification.

# Gradient Boost > Random Forest > Decision Tree

# Stacking Model
"""

from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression

models_list = [
    ('dtree',dtree_clf),
    ('random_forest', random_forest_clf),
    ('xgboost', xgboost_clf)]

# Build stack model
stack_ml_model = StackingClassifier(
    estimators=models_list, final_estimator= GradientBoostingClassifier()
)

# Train stacked model
stack_ml_model.fit(X_train, y_train)

#making predictions on test_set
y_test_pred_stacked = stack_ml_model.predict(X_test)

# Test set model performance
stacked_model_test_accuracy = accuracy_score(y_test, y_test_pred_stacked) # Calculate Accuracy
stacked_model_test_mcc = matthews_corrcoef(y_test, y_test_pred_stacked) # Calculate MCC
stacked_model_test_f1 = f1_score(y_test, y_test_pred_stacked, average='weighted') # Calculate F1-score

print('Model performance of Stcaked Model for Test set')
print('- Accuracy: %s' % stacked_model_test_accuracy)
print('- MCC: %s' % stacked_model_test_mcc)
print('- F1 score: %s' % stacked_model_test_f1)

"""# Saving the model"""

#using joblib
import joblib

model_file_tree = open("decision_tree_model.pkl", "wb")
joblib.dump(dtree_clf, model_file_tree)
model_file_tree.close()

model_file_rf = open("random_forest_model.pkl", "wb")
joblib.dump(random_forest_clf, model_file_rf)
model_file_rf.close()

model_file_xg = open("xgboost_model.pkl", "wb")
joblib.dump(xgboost_clf, model_file_xg)
model_file_xg.close()

"""## Using Sklearn's Pipeline"""

from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

def pipeline_data(df):
  df = df.copy()

  #Replacing categorical column records with 0 and 1 (Only binary columns)
  df['Gender'] = df['Gender'].replace(['Male', 'Female'], [1,0])
  df['family_history_with_overweight'] = df['family_history_with_overweight'].replace(['yes', 'no'], [1,0])
  df['FAVC'] = df['FAVC'].replace(['yes', 'no'], [1,0])
  df['SMOKE'] = df['SMOKE'].replace(['yes', 'no'], [1,0])
  df['SCC'] = df['SCC'].replace(['yes', 'no'], [1,0])

  #X and y
  X = df.drop("NObeyesdad", axis=1)
  y = df['NObeyesdad']

  #split
  
  X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.8, shuffle=True, random_state=21)
 
  return X_train, X_test, y_train, y_test

X_train, X_test, y_train, y_test = pipeline_data(data)

X_train

y_train

"""# Pipeline"""

categorical_transformer = Pipeline(steps=[
                                          ('onehot', OneHotEncoder(sparse=False))
])

column_transformer = ColumnTransformer(transformers=[
                                                     ('categorical', categorical_transformer,
                                                      ['CAEC', 'CALC', 'MTRANS'])
], remainder='passthrough')

model = Pipeline(steps=[
                        ('column', column_transformer),
                        ('classifier', GradientBoostingClassifier())
])

model.fit(X_train, y_train)

"""# Evaluation"""

score = model.score(X_test, y_test)
print("Model score is:", np.round(score*100), "%")

y_pred_pipeline = model.predict(X_test)
print(y_pred_pipeline)

plot_confusion_matrix(model, X_test, y_pred_pipeline, labels= model.classes_)

pipeline_clr = classification_report(y_test, y_pred_pipeline, labels=model.classes_)
print(pipeline_clr)

