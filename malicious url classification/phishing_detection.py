# -*- coding: utf-8 -*-
"""phishing detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mERwGn_CUKWYd29l6DFUDoszvZJvCC0_
"""

import pandas as pd

data = pd.read_excel("drive/My Drive/malicious url/data_new.xlsx")

data

data.isna().sum()

data.dropna(subset=['url'], how='all', inplace=True)

data.dropna(subset=['label'], how='all', inplace=True)

data.isna().sum()

data.shape

import plotly.express as px

fig = px.pie(data['label'], values=data['label'].value_counts().values, names=data['label'].value_counts().index)
fig.update_traces(hoverinfo='label+percent', textinfo='value')
fig.show()

"""## Default Tokenizer"""

import numpy as np 
import random

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

label = data['label']

feature = data['url']

deafult_vectorizer = TfidfVectorizer()

X = deafult_vectorizer.fit_transform(feature)

X_train, X_test, y_train, y_test = train_test_split(X, label, test_size=0.3, shuffle=True, random_state=1)

model = LogisticRegression()
model.fit(X_train, y_train)

# Accuracy
print("Accuracy ", model.score(X_test, y_test))

predicted = model.predict(X_test)

predicted

from sklearn.metrics import confusion_matrix
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, plot_confusion_matrix
import matplotlib.pyplot as plt

plot_confusion_matrix(model, X_test, predicted)

import pickle
# save the model to disk
filename = "drive/My Drive/malicious url/default_model.pkl"
pickle.dump(model, open(filename, 'wb'))

"""## using custom tokenizer"""

our_feature = data['url']
our_label = data['label']

def makeTokens(f):
    tkns_BySlash = str(f.encode('utf-8')).split('/')	# make tokens after splitting by slash
    total_Tokens = []
    for i in tkns_BySlash:
      tokens = str(i).split('-')	# make tokens after splitting by dash
      tkns_ByDot = []
      for j in range(0,len(tokens)):
        temp_Tokens = str(tokens[j]).split('.')	# make tokens after splitting by dot
        tkns_ByDot = tkns_ByDot + temp_Tokens
      total_Tokens = total_Tokens + tokens + tkns_ByDot
    total_Tokens = list(set(total_Tokens))	#remove redundant tokens
    if 'com' in total_Tokens:
      total_Tokens.remove('com')	#removing .com since it occurs a lot of times and it should not be included in our features
    elif 'net' in total_Tokens:
      total_Tokens.remove('net')  #removing .net since it occurs a lot of times and it should not be included in our features
    return total_Tokens

custom_vectorizer = TfidfVectorizer(tokenizer=makeTokens)

custom_X = custom_vectorizer.fit_transform(our_feature)

custom_X_train, custom_X_test, custom_y_train, custom_y_test = train_test_split(custom_X, our_label, test_size=0.3, shuffle=True, random_state=42)

custom_model = LogisticRegression()
custom_model.fit(custom_X_train, custom_y_train)

# Accuracy of Our Model
print("Accuracy ", custom_model.score(custom_X_test, custom_y_test))

custom_predicted = custom_model.predict(custom_X_test)
custom_predicted

plot_confusion_matrix(custom_model, custom_X_test, custom_predicted)