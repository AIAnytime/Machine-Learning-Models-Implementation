# -*- coding: utf-8 -*-
"""Fraudulent Firms Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tX322i8WekTWLd-TLqDAEupBNDsuvTnQ
"""

# Commented out IPython magic to ensure Python compatibility.
#import trio
import pandas as pd
pd.set_option('max_columns', None)
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline

#data preparation pkgs
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

#classifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC, LinearSVC
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier

#metrics
from sklearn.metrics import classification_report, plot_confusion_matrix, confusion_matrix

"""## Widget to upload the data file"""

from google.colab import files
files.upload()

"""# Data Source: Dataset is available on Kaggle.
Find here: https://www.kaggle.com/sid321axn/audit-data
"""

data = pd.read_csv("audit_data.csv")

data

!pip install pycaret













"""## EDA

## Check Missing value
"""

#check missing value
data.isna().sum()

data.info()

#fill the missing value in Money_Value column
data['Money_Value'] = data['Money_Value'].fillna(data['Money_Value'].mean())

data.isna().sum()

"""## So, no missing value now as we filled the 1 missing value with the mean of the column as it is a numerical column."""

#Plotting count values of Positive and Negative

data['Risk'].value_counts().plot(kind = 'bar')

"""### 1 means Fraudulent and 0 means Not Fraudulent

#Feature Engineering
"""

#Let's create a copy of our main data
def data_preparation(df):
  df = df.copy()
  return df

"""#### We will do all the processing and feature engineering step on the new_data"""

new_data = data_preparation(data)

new_data

#check the mising value once more
new_data.isna().sum()

location_dummies = pd.get_dummies(new_data['LOCATION_ID'], prefix='location')

new_data = pd.concat([new_data, location_dummies], axis=1)

new_data = new_data.drop('LOCATION_ID', axis=1)

new_data

X = new_data.drop("Risk", axis=1)
y = new_data['Risk']

X

y

#Split the data
X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.7, shuffle=True, random_state=21)

print(len(X_train))
print(len(X_test))
print(len(y_train))
print(len(y_test))

X_train

y_test

#Scale the data to make it in the same range
# Only the X_train 
scaler = StandardScaler()
scaler.fit(X_train)
X_train = pd.DataFrame(scaler.transform(X_train), index=X_train.index, columns=X_train.columns)
X_test = pd.DataFrame(scaler.transform(X_test), index=X_test.index, columns=X_test.columns)

X_train

X_test

y_train

"""## Model Training"""

logreg = LogisticRegression()
logreg_clf = logreg.fit(X_train, y_train)

d_tree = DecisionTreeClassifier()
d_tree_clf = d_tree.fit(X_train, y_train)

random_forest = RandomForestClassifier()
random_forest_clf = random_forest.fit(X_train, y_train)

xgboost = GradientBoostingClassifier()
xgboost_clf = xgboost.fit(X_train, y_train)

svm = SVC()
svm_clf = svm.fit(X_train, y_train)

linear_kernel = LinearSVC()
linear_kernel_clf = linear_kernel.fit(X_train, y_train)

"""# Evaluation"""

#Logistic Regression
score_log = logreg_clf.score(X_test, y_test)
score_d_tree = d_tree_clf.score(X_test, y_test)
score_random_forest = random_forest_clf.score(X_test, y_test)
score_xgboost = xgboost_clf.score(X_test, y_test)
score_svm = svm_clf.score(X_test, y_test)
score_kernel = linear_kernel_clf.score(X_test, y_test)
print("LOGISTIC REGRESSION:", score_log*100, "%")
print("Decision Tree:", score_d_tree*100, "%")
print("Random Forest:", score_random_forest*100, "%")
print("XGBOOST:", score_xgboost*100, "%")
print("SVM:", score_svm*100, "%")
print("LINEAR KERNEL:", score_kernel*100, "%")

y_pred_logreg = logreg_clf.predict(X_test)
y_pred_dtree = d_tree_clf.predict(X_test)
y_pred_random = random_forest_clf.predict(X_test)
y_pred_xgboost = xgboost_clf.predict(X_test)
y_pred_svm = svm_clf.predict(X_test)
y_pred_linear = linear_kernel_clf.predict(X_test)

clf_logreg = classification_report(y_test, y_pred_logreg)
print(clf_logreg)

plot_confusion_matrix(logreg_clf, X_test, y_test)

clf_dtree = classification_report(y_test, y_pred_dtree)
print(clf_dtree)

plot_confusion_matrix(d_tree_clf, X_test, y_test)

clf_random = classification_report(y_test, y_pred_random)
print(clf_random)

plot_confusion_matrix(random_forest_clf, X_test, y_test)

clf_xgboost = classification_report(y_test, y_pred_xgboost)
print(clf_xgboost)

plot_confusion_matrix(xgboost_clf, X_test, y_test)

clf_svm = classification_report(y_test, y_pred_svm)
print(clf_svm)
plot_confusion_matrix(svm_clf, X_test, y_test)

clf_kernel = classification_report(y_test, y_pred_linear)
print(clf_kernel)
plot_confusion_matrix(linear_kernel_clf, X_test, y_test)