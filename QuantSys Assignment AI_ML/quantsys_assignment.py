# -*- coding: utf-8 -*-
"""QuantSys Assignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HoS1rGnPinObs12e8dQJoYr-2r6iU4aV
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns

data = pd.read_excel("Training Set.xlsx")

data

"""## Basic EDA"""

data.info()

data.describe()

"""### Correlation Matrix Plot"""

corr = data.corr()
ax = sns.heatmap(
    corr, 
    vmin=-1, vmax=1, center=0,
    cmap=sns.diverging_palette(20, 220, n=200),
    square=True
)
ax.set_xticklabels(
    ax.get_xticklabels(),
    rotation=45,
    horizontalalignment='right'
);

"""#### Got a good diagonal!!

## Checking Unique Values for Onehot Encode
"""

data['Is Working Day'].unique()

data['Weekday'].unique()

"""## Checking Missing Values"""

data.isna().sum()

"""## Narrative:
- As all the columns are numerical but  "Is Working Day", "Weekday", needs to be one hot encoded as they are categorical by nature.

- I will extract features from Date column using datetime object.

- No Missing Values

- target is the target column and I will extract all others features with the current features.

#"REGRESSION TASK"
Let's start!!

## I will do this using 2 different ways:
- Sklearn's Pipeline
- PyCaret

# Using Pipeline
"""

from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

from sklearn.ensemble import GradientBoostingRegressor

from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

"""## Data Preparation Workflow:
- Extract features from Date Column using Dateobject. I am keeping in mind that I will use this model for future predictions so I wil drop Year in Date column because that will not make any sense for future predictions. Month, Day, and Hour will play an important role in the model performance.

- X and y

- Split X and y

- Tree-based models don't require scaled data so I will not use StandardScaler in this case of the Pipeline.

- month will be a categorical feature again so we need to onehot encode that.
"""

def preprocess_inputs(df):
  df = df.copy()
  #Extract month, day, and hour using Datetime object
  df['Date'] = pd.to_datetime(df['Date']) #datetime object
  df['month'] = df['Date'].apply(lambda x: x.month) #extract month from datetime object
  df['day'] = df['Date'].apply(lambda x: x.day) #extract day from the datetime object
  df['hour'] = df['Date'].apply(lambda x: x.hour) #extract hour from the datetime object

  #Now I can drop the original Date column
  df = df.drop("Date", axis=1)

  # X and y
  X = df.drop("target", axis=1)
  y = df['target']

  #Split
  X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.7, shuffle=True, random_state=1)
  
  return X_train, X_test, y_train, y_test

X_train, X_test, y_train, y_test = preprocess_inputs(data)

X_train

y_train

print(len(X_train))
print(len(X_test))
print(len(y_train))
print(len(y_test))

"""## Build the pipeline"""

nominal_transformer = Pipeline(steps=[
                                      ('onehot', OneHotEncoder(sparse=False))
])

preprocessor = ColumnTransformer(transformers=[
                                               ('nominal', nominal_transformer, ['Weekday', 'Is Working Day', 'month'])
], remainder='passthrough')

model = Pipeline(steps=[
                        ('preprocessor', preprocessor),
                        ('regressor', GradientBoostingRegressor())
])

estimator = model.fit(X_train, y_train)

"""# Evaluation"""

y_true = np.array(y_test)
print(y_true)
print("----------------------------------")
y_pred = estimator.predict(X_test)
print(y_pred)

print("Model R^2 Score: {:.4f}".format(r2_score(y_true, y_pred)))

## RMSE

print("MSE is:",np.mean((y_test - y_pred) ** 2))

rmse = np.sqrt(np.mean((y_test - y_pred) ** 2))
print("RMSE is:", rmse)

import pickle

# save the model to disk
filename = 'regression_model.pkl'
pickle.dump(estimator, open(filename, 'wb'))

"""## Using PyCaret"""

!pip install pycaret

import pycaret.regression as pyr

def data_preparation(df):
  df = df.copy()

  #Extract month, day, and hour using Datetime object
  df['Date'] = pd.to_datetime(df['Date']) #datetime object
  df['month'] = df['Date'].apply(lambda x: x.month) #extract month from datetime object
  df['day'] = df['Date'].apply(lambda x: x.day) #extract day from the datetime object
  df['hour'] = df['Date'].apply(lambda x: x.hour) #extract hour from the datetime object

  #Now I can drop the original Date column
  df = df.drop("Date", axis=1)


  return df

X = data_preparation(data)

X

pyr.setup(
    data = X,
    target= 'target',
    train_size=0.7,
    normalize = True
)

pyr.compare_models()

best_model = pyr.create_model('et')

pyr.evaluate_model(best_model)

test_data = pd.read_excel("Test Set.xlsx")

test_data

def test_data_for_prediction(df):
  df = df.copy()
  #Extract month, day, and hour using Datetime object
  df['Date'] = pd.to_datetime(df['Date']) #datetime object
  df['month'] = df['Date'].apply(lambda x: x.month) #extract month from datetime object
  df['day'] = df['Date'].apply(lambda x: x.day) #extract day from the datetime object
  df['hour'] = df['Date'].apply(lambda x: x.hour) #extract hour from the datetime object

  #Now I can drop the original Date column
  df = df.drop(['Date', 'target'], axis=1)

  return df

unseen_data = test_data_for_prediction(test_data)

unseen_data

predictions = pyr.predict_model(best_model, data=unseen_data)

predictions

predictions.to_excel("predictions_result.xlsx")

"""## You can find all the predictions results inside the excel file on the given test data."""